[ğŸ“š Back to Table of Contents](../../README.md)

# 1.2. Handling Hallucinations and Distortions in AI Responses

One of the most dangerous yet subtle meta-problems is **AI hallucinations** â€” situations when a language model confidently makes up facts, terms, references, definitions, or logical connections that donâ€™t exist.

This is not a bug in the code or an act of malice â€” it is a **fundamental feature** of how text generation works. The model doesn't â€œknowâ€ the truth; it simply generates the most statistically likely sequence of words based on the context and style of your prompt.

## ğŸ“Š Table 1 â€” Examples of AI Hallucinations

| Type                      | Example                                                             | Comment                                                                 |
|---------------------------|---------------------------------------------------------------------|-------------------------------------------------------------------------|
| Invented source           | â€œAccording to a 2021 Harvard studyâ€¦â€                                | Sounds authoritative, but no such study exists.                        |
| Fake quote                | â€œAs Steve Jobs said: â€˜Design is when beauty meets simplicity.â€™â€     | Feels like Jobs, but he never said it.                                 |
| Illogical reasoning       | â€œSince water freezes at 0Â°C, that explains air behavior at -10Â°C.â€  | Formally logical, but the connection is imaginary.                     |
| Distorted known facts     | â€œUnemployment rate in Germany in 2023 was 18%.â€                     | The actual figure was around 5â€“6%.                                     |
| Fictional term or method  | â€œReverse Cognitive Activation Methodâ€                               | Not a real method, but sounds scientific.                              |

---

## ğŸ¤– Why Does It Happen?

The model does **not access a database of facts**. It doesnâ€™t search Wikipedia or books. Instead:
- It predicts the next likely word;
- If you ask for a â€œcitation,â€ it mimics the appearance of citations;
- If you ask for reasoning, it follows common argumentative templates.

The main criterion is not **truth**, but **coherence and plausibility**.

---

## âš  Why Is This Dangerous?

- â— You might believe it â€” especially if you donâ€™t double-check.
- â— You might use fake info in work, school, or publications.
- â— Errors are polished â€” hallucinations often look beautifully convincing.

---

## âœ… How to Work with This Issue

1. **Verify confident statements**
   - If the model says:  
     â€œAccording to WHO data...â€ or â€œIt is well known that...â€ â€” ask yourself:  
     _Can I actually find this source online?_

2. **Ask meta-questions directly**
   - â€œWhat is this based on?â€  
   - â€œIs there a source?â€  
   - â€œWhatâ€™s the likelihood of error here?â€

3. **Use external verification**
   - Google, Wikipedia, or another AI model
   - This can:
     - prevent falsehoods from spreading;
     - bring real depth to your understanding;
     - train your skill to spot pseudo-knowledge.

4. **Rephrase your prompt**
   - â€œProvide verifiable sources on this topic.â€  
   - â€œAre there any known studies that support this?â€  
   - â€œGive an example â€” but warn me if itâ€™s fictional.â€

5. **Ask for a hypothesis â€” if thatâ€™s what you want**
   - â€œCreate a possible version or fantasy about...â€  
   - â€œBuild a scenario, even if hypothetical.â€

By framing your prompt explicitly, you avoid the illusion: â€œIt sounds convincing, so it must be true.â€

---

## ğŸ“‹ Checklist: Spotting AI Hallucinations

| Question                                                        | Sign of Hallucination |
|------------------------------------------------------------------|------------------------|
| Does it sound too polished or too confident?                     | Yes                    |
| Is there no specific source?                                     | Yes                    |
| Is the citation or quote untraceable but sounds persuasive?      | Yes                    |
| Is the model certain, but gives no context or explanation?       | Yes                    |
| Is the topic complex and interdisciplinary?                      | Higher risk            |

[â¬…ï¸ Chapter 1.1.](chapter11.md)  |  [Chapter 1.3. â¡ï¸](chapter13.md)
